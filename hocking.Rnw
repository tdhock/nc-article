\newcommand{\sectiontidyr}{Comparisons with other functions for data frames}
\newcommand{\sectiontrackDb}{Capturing all matches from a multi-line text file}
\newcommand{\sectiontimings}{Comparing computation times of R regex packages}
\newcommand{\sectiondf}{Creating new columns from character columns in a data frame}
\newcommand{\sectionrex}{Comparing \pkg{namedCapture} variable argument syntax with \pkg{rex}}
\newcommand{\sectioncomparisons}{Comparisons with other R packages}

\title{Regular expressions and reshaping using data tables and the
  \pkg{nc} package}

\author{by Toby Dylan Hocking}

\maketitle

\abstract{Regular expressions are powerful tools for extracting tables
  from non-tabular text data. Capturing regular expressions that
  describe information to extract from column names can be especially
  useful when reshaping a data table from wide (one row with many
  columns) to tall (one column with many rows). We present the R
  package \pkg{nc} (short for named capture), which provides functions
  for data reshaping, regular expressions, and a uniform interface to
  three C libraries (PCRE, RE2, ICU). We describe the main features of
  \pkg{nc}, then provide detailed comparisons with related R packages
  (\pkg{stats}, \pkg{utils}, \pkg{data.table}, \pkg{tidyr},
  \pkg{reshape2}, \pkg{cdata}).}

\section{Introduction}

Regular expressions are powerful tools for text processing that are
available in many programming languages, including R. A regular
expression \dfn{pattern} defines a set of \dfn{matches} in a
\dfn{subject} string. For example, the pattern \code{.*[.].*} matches
zero or more non-newline characters, followed by a period, followed by
zero or more non-newline characters. It would match the subjects
\code{Sepal.Length} and \code{Petal.Width}, but it would not match in
the subject \code{Species}.

The focus of this article is patterns with capture groups, which are
typically defined using parentheses. For example, the pattern
\code{(.*)[.](.*)} results in the same matches as the pattern in the
previous paragraph, and it additionally allows the user to capture and
extract the substrings by group index (e.g. group 1 matches
\code{Sepal}, group 2 matches \code{Length}).

Named capture groups allow extracting the a substring by name rather
than by index. Using names rather than indices is useful in order to
create more readable regular expressions (names document the purpose
of each sub-pattern), and to create more readable \R\ code (it is
easier to understand the intent of named references than numbered
references). For example, the pattern
\code{(?<part>.*)[.](?<dimension>.*)} documents that the flower part
appears before the measurement dimension; the \code{part} group
matches \code{Sepal} and the \code{dimension} group matches
\code{Length}.

Recently, \citet{HOCKING2019-namedCapture} proposed a new syntax for
defining named capture groups in R code. Using this new syntax,
named capture groups are specified using named arguments in R,
which results in code that is easier to read and modify than
capture groups defined in string literals. For example, the
pattern in the previous paragraph can be written as \code{part=".*",}
\code{"[.]",} \code{dimension=".*"}.
Sub-patterns can be grouped for
clarity and/or re-used using
lists, and numeric data may be extracted with user-provided
type conversion functions.

A main thesis of this article is that regular expressions can greatly
simplify the code required to specify wide-to-tall data reshaping
operations. For one such operation the input is a ``wide'' table with
many columns, and the desired output is a ``tall'' table with more
rows, and some of the input columns converted into a smaller number of
output columns (Figure~\ref{fig:wide-to-tall}). To clarify the
discussion we first define three terms that we will use to refer to
the different types of columns involved in this conversion:
\begin{description}
\item[Reshape] columns contain the data which is present in the same
  amount but in different shapes in the input and output. There are
  equivalent terms used in different R packages: \code{varying} in
  \code{utils::reshape}, \code{measure.vars} in \code{melt}
  (\pkg{data.table}, \pkg{reshape2}), etc.
\item[Copy] columns contain data in the input which are each copied to
  multiple rows in the output (\code{id.vars} in \code{melt}).
\item[Capture] columns are only present in the output, and contain
  data which come from matching a
  capturing regex pattern to the input reshape column names.
\end{description}
For example the wide iris data (W in Figure~\ref{fig:wide-to-tall})
have four numeric columns to reshape: \code{Sepal.Length},
\code{Sepal.Width}, \code{Petal.Length}, \code{Petal.Width}. For some
purposes (e.g. displaying a histogram of each reshape input column
using facets in \CRANpkg{ggplot2}) the desired reshaping operation
results in a table with a single reshape output column (S in
Figure~\ref{fig:wide-to-tall}), two copied columns, and two columns
captured from the names of the reshaped input columns. For other
purposes (e.g. scatterplot to compare Petal and Sepal sizes) the
desired reshaping operation results in a table with multiple reshape
output columns (M1 with \code{Sepal} and \code{Petal} columns in
Figure~\ref{fig:wide-to-tall}), two copied columns, and one column
captured from the names of the reshaped input columns. We propose to
use the new regular expression syntax of
\citet{HOCKING2019-namedCapture}, e.g. \code{part=".*",} \code{"[.]",}
\code{dimension=".*"}, to define both types of wide-to-tall data
reshaping operations. In particular, we propose using a single
capturing regular expression for defining both (1) the subset of
reshape input columns to convert, and (2) the additional capture
output columns. We will show that this results in a simple, powerful,
non-repetitive syntax for wide-to-tall data reshaping.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{figure-1-iris}
  \caption{Two rows of the iris data set (W, black) are considered as
    the input to a wide-to-tall reshape operation. Four input reshape
    columns are converted to either a single output reshape column (S,
    blue) or multiple (2) output reshape columns (M1, M2, red). Other
    output columns are either copied from the non-reshaped input data,
    or captured from the names of the reshaped input columns.}
  \label{fig:wide-to-tall}
\end{figure}

In this article our original contribution is the \R\ package
\CRANpkg{nc} which provides a new implementation of the previously
proposed named capture regex syntax of
\citet{HOCKING2019-namedCapture}, in addition to several new functions
that perform wide-to-tall data reshaping using regular
expressions. The main new ideas are (1) using un-named capture groups
in the regex string literal to provide a uniform interface to three
regex C libraries, (2) integration of capture groups and
\CRANpkg{data.table} functionality \citep{Dowle2019}, and (3)
specifying wide-to-tall reshape operations with a concise syntax which
results in less repetitive user code than other packages. A secondary
contribution of this article is a detailed comparison of current R
functions for reshaping data with regular expressions.

The organization of this article is as follows. The rest of this
introduction provides an overview of current \R\ packages for regular
expressions and data reshaping. The second section describes the
proposed functions of the \CRANpkg{nc} package. The third section provides
detailed comparisons with other \R\ packages, in terms of syntax and
computation times. The article concludes with a summary and
discussion.

\section{Related work}

There are many R functions which can extract tables from non-tabular
text using regular expressions. Recommended R package functions include \code{base::regexpr}
and \code{base::gregexpr} as well as \code{utils::strcapture}. CRAN
packages include
\CRANpkg{namedCapture} \citep{namedCapture},
\CRANpkg{rematch2} \citep{rematch2},
\CRANpkg{rex} \citep{rex},
\CRANpkg{stringr} \citep{stringr},
\CRANpkg{stringi} \citep{stringi},
\CRANpkg{tidyr} \citep{tidyr},
and
\CRANpkg{re2r} \citep{re2r}.
\citet{HOCKING2019-namedCapture} provides a detailed comparison of these packages
in terms of features, syntax, and computation time.

For reshaping data from wide (one row with many columns) to tall (one
column with many rows), there are several different R functions that
provide similar functionality. Each function supports a different set
of features (Table~\ref{tab:features}); each feature/column is
explained in detail below:
\begin{description}
\item[single] refers to support for converting input reshape columns
  of the same type to a single reshape output column.
\item[multiple] refers to support for converting input reshape columns of
  possibly different types to multiple output reshape columns; ``sorted''
  means that conversion works correctly only if the input reshape columns are sorted
  in a regular order, e.g. \code{Sepal.Length}, \code{Sepal.Width},
  \code{Petal.Length}, \code{Petal.Width}; ``unsorted'' means that
  conversion works correctly even if the they are not sorted,
  e.g. \code{Sepal.Length}, \code{Sepal.Width}, \code{Petal.Width},
  \code{Petal.Length}.
\item[regex] refers to support for regular expressions; ``match''
  means a pattern is used to match the input column names; ``capture''
  means that the specified pattern is used to
  create new output capture columns --- this is especially useful when the names
  consist of several distinct pieces of information, e.g. \code{Sepal.Length};
  ``no'' means that regular expressions are not directly supported
  (although \code{base::grep} can always be used).
\item[na.rm] refers to support for removing missing values.
\item[types] refers to support for converting captured text to numeric
  output columns.
\item[list] refers to support for output of list columns.
\end{description}


\begin{table}
  \centering
  \begin{tabular}{llllllll}
\toprule
\code{pkg::function} & single & multiple & regex & na.rm & types & list \\
\midrule
\code{nc::capture\_melt\_multiple} & no & unsorted & capture & yes & any & yes\\
\code{nc::capture\_melt\_single} & yes & no & capture & yes & any & yes\\
\code{tidyr::pivot\_longer} & yes & unsorted & capture & yes & some & yes\\
\code{stats::reshape} & yes & sorted & capture & no & some & no\\
\code{data.table::melt}, \code{patterns} & yes & sorted & match & yes & no & yes\\
\code{tidyr::gather} & yes & no & no & yes & some & yes\\
\code{reshape2::melt} & yes & no & no & yes & no & no\\
\code{cdata::rowrecs\_to\_blocks} & yes & unsorted & no & no &no & yes\\
\code{cdata::unpivot\_to\_blocks} & yes & no & no & no &no & yes\\
\code{utils::stack} & yes & no & no & no & no & no\\
\bottomrule
    \end{tabular}
    \caption{\label{tab:features}
      Reshaping functions in R support various features:
      ``single'' for converting input columns into a single output column;
      ``multiple'' for converting input columns
      (either ``sorted'' in a regular order, or ``unsorted'' for any order)
      into multiple output columns of different types;
      ``regex'' for regular expressions to
      ``match'' input column names or to
      ``capture'' and create new output column names;
      ``na.rm'' for removal of missing values;
      ``types'' for converting input column names to non-character output columns;
      ``list'' for output of list columns.
    }
\end{table}

Recommended R package functions include \code{stats::reshape} and
\code{utils::stack} for reshaping data from wide to tall. Of the
features listed in Table~\ref{tab:features}, \code{utils::stack} only
supports output with a single reshape column, whereas \code{stats::reshape} supports
the following features. For data with regular input column names (output
column, separator, time value), regular expressions can be used to
specify the separator (e.g. in \code{Sepal.Length}, \code{Sepal} is
output column, dot is separator, \code{Length} is time
value). Multiple output columns are supported, but incorrect output
may be computed if input columns are not sorted in a regular
order. The time value is output to a capture column named
\code{time} by default. Automatic type conversion is performed on time values
when possible, but custom type conversion functions are not
supported. There is neither support for missing value removal nor list
column output.

The \pkg{tidyr} package provides two functions for reshaping data from
wide to tall format: \code{gather} and \code{pivot\_longer}. The older
\code{gather} function only supports converting input reshape columns to a
single output reshape column (not multiple).
The input reshape columns to convert may
not be directly specified using regular expressions; instead R
expressions such as \code{x:y} can be used to indicate all columns
starting from \code{x} and ending with \code{y}. It does support
limited type conversion; if the \code{convert=TRUE} argument is
specified, the \code{utils::type.convert} function is used to convert
the input column names to numeric, integer, or logical. In contrast
the newer \code{pivot\_longer} also supports multiple output reshape columns
(even if input reshape columns are unsorted), and regular expressions for
specifying output capture columns
(but to specify input reshape columns with a regex,
\code{grep} must be used). Limited type conversion is also supported in
\code{pivot\_longer}, via the \code{names\_ptypes} argument, which
should be a list with names corresponding to output columns and values
corresponding to prototypes (zero-length atomic vectors,
e.g. \code{numeric()}). Both functions support list columns and
removing missing values, although different arguments are used
(\code{na.rm} for \code{gather}, \code{values\_drop\_na} for
\code{pivot\_longer}).

The \CRANpkg{reshape2} and \pkg{data.table} packages each provide a
\code{melt} function for converting data from wide to tall
\citep{Wickham2007, Dowle2019}. The older \pkg{reshape2} version only
supports converting input reshape columns to a single output reshape column, whereas
the newer \pkg{data.table} version also supports multiple output reshape
columns. Regular expressions are not supported in \pkg{reshape2}, but
can be used with \code{data.table::patterns} to match input column
names to convert (although the output can be incorrect if columns are
not sorted in a regular order). Neither function supports type
conversion, and both functions support removing missing values from
the output using the \code{na.rm} argument. List column output is
supported in \pkg{data.table} but not \pkg{reshape2}.

The \CRANpkg{cdata} package provides several functions for data
reshaping, including \code{rowrecs\_to\_blocks} and
\code{unpivot\_to\_blocks} which can convert data from wide to tall
\citep{Mount2019}. The simpler of the two functions is
\code{unpivot\_to\_blocks}, which supports a single output reshape column
(interface similar to \code{reshape2::melt}/\code{tidyr::gather}). The
user of \code{rowrecs\_to\_blocks} must provide a control table that
describes how the input should be reshaped into the output. It
therefore supports multiple output reshape columns, for possibly unsorted
input columns. Both functions support list column output, but other
features from Table~\ref{tab:features} are not supported (regular
expressions, missing value removal, type conversion).

\section{New features in \pkg{nc} for regular expressions and data reshaping}

The \pkg{nc} package provides new regular expression functionality
based on the syntax recently proposed by
\citet{HOCKING2019-namedCapture}. In this section we first discuss how
the \pkg{nc} package implements this syntax as a front-end to three
regex engines, and we then discuss the new features for data reshaping
using regular expressions.

\subsection{Uniform interface to three regex engines}

Several C libraries providing regular expression engines are available
in R. The standard R distribution has included and the Perl-Compatible
Regular Expressions (PCRE) C library since 2002 \citep{R.NEWS.1.txt}.
CRAN package \pkg{re2r} provides the RE2 library, and \pkg{stringi}
provides the ICU library. Each of these regex engines has a unique
feature set, and may be preferred for different applications. For
example, PCRE is installed by default, RE2 guarantees matching in
polynomial time, and ICU provides strong unicode support. For a more
detailed comparison of the relative strengths of each regex library,
we refer the reader to the recent paper of
\citep{HOCKING2019-namedCapture}.

Each regex engine has also a different R interface, so switching from
one engine to another may require non-trivial modifications of user
code. In order to make switching between engines easier,
\citet{HOCKING2019-namedCapture} introduced the \pkg{namedCapture}
package, which provides a uniform interface for capturing text using
PCRE and RE2. The user may specify the desired engine via
e.g. \code{options(namedCapture.engine="PCRE")}; the
\pkg{namedCapture} package provides the output in a uniform
format. However \pkg{namedCapture} requires the engine to support
specifying capture group names in regex pattern strings, and to
support output of the group names to R (which ICU does not support).

Our proposed \pkg{nc} package provides support for the ICU engine in
addition to PCRE and RE2. The \pkg{nc} package implements this
functionality using un-named capture groups, which are supported in
all three regex engines. In particular, a regular expression is
constructed in R code that uses named arguments to indicate captures,
which are translated to un-named groups when passed to the regex
engine. For example, consider a user who wants to capture the two
pieces of the column names of the iris data,
e.g. \code{Sepal.Length}. The user would typically specify the
capturing regular expression as a string literal,
e.g. \code{"(.*)[.](.*)"}.
Using \pkg{nc} the same pattern can be
applied to the iris data column names via

<<capture-iris-cols>>=
nc::capture_first_vec(
  names(iris), part=".*", "[.]", dim=".*", engine="ICU", nomatch.error=FALSE)

@

Above we see an example usage of \code{nc:capture\_first\_vec}, which
is for capturing the first match of a regex from a character vector
subject (the first argument). There are a variable number of other
arguments (\code{...}) which are used to define
the regex pattern. In this case there are three pattern arguments:
\code{part=".*", "[.]", dim=".*"}. Each named R argument in the
pattern generates an un-named capture group by enclosing the specified
value in parentheses, e.g. \code{(.*)}. All of the sub-patterns are
pasted together in the sequence they appear in order to create the
final pattern that is used with the specified regex engine. The
\code{nomatch.error=FALSE} argument is given because the default is to
stop with an error if any subjects do not match the specified pattern
(the fifth subject \code{Species} does not match). Under the hood, the
following function is called to parse the pattern arguments:

<<see>>=
str(compiled <- nc::var_args_list(part=".*", "[.]", dim=".*"))

@

This function is intended mostly for internal use, but can be useful
for viewing the generated regex pattern (or using it as input to
another regex function). The return value is a named list of two
elements: \code{pattern} is the capturing regular expression which is
generated based on the input arguments, and \code{fun.list} is a named
list of type conversion functions. Group-specific type conversion functions
are useful for converting captured text into numeric output columns
\citet{HOCKING2019-namedCapture}. Note that the order of elements in
\code{fun.list} corresponds to the order of capture groups in the
pattern (e.g. first capture group named \code{part}, second
\code{dim}). These data can be used with any regex engine that
supports un-named capture groups (including ICU) in order to get a
capture matrix with column names, e.g.

<<icu>>=
m <- stringi::stri_match_first_regex(names(iris), compiled$pattern)
colnames(m) <- c("match", names(compiled$fun.list))
m

@

Again, this is not the recommended usage of \pkg{nc}, but here we give
these details in order to explain how it works. Note that the result
from \pkg{stringi} is a character matrix with three columns: first for
the entire match, and another column for each capture group. Using the
same pattern with \code{base::regexpr} (PCRE engine) or
\code{re2r::re2\_match} (RE2 engine) yields output in varying formats.
The \pkg{nc} package takes care of converting these different results
into a standard format which makes it easy to switch regex engines (by
changing the value of the \code{engine} argument).

Note that the standard format used by \pkg{nc}, as shown above with
\code{nc::capture\_first\_vec}, is a data table. The main reason that
data tables are always output by \pkg{nc} matching functions is in
order to support numeric output columns, when type conversion
functions are specified. A secondary reason is for the convenient
syntax, as we will show in the next section.

\subsection{Data table integration and
  \code{nc::field} to avoid repetition}

In the previous section we mentioned that every \pkg{nc} matching
function outputs a data table, which can be
queried/summarized/joined/etc using the convenient syntax of the
\pkg{data.table} package. In this section we show an example of how
this syntax makes is easy to parse non-tabular text output files from
a bioinformatics program.  We also show an example of how the new
\code{nc::field} function can be used to avoid repetition when
defining a pattern to match fields of the form \code{variable=value}.

We consider a bioinformatics program called SweeD which outputs two
data files that are linked by an integer alignment ID number
\citep{SweeD}. The ``Report'' file is similar to tab-separated values
(TSV), except that there are some blank lines, followed by two forward
slashes, followed by the integer alignment number. For example, the
first few lines of the first two alignments look like:

<<sweedReport>>=
report.txt.gz <- system.file("extdata", "SweeD_Report.txt.gz", package="nc")
report.vec <- readLines(report.txt.gz)
cat(report.vec[1:5], sep="\n")
cat(report.vec[1003:1008], sep="\n")

@

The file above can be easily parsed by \code{nc::capture\_all\_str},
which is for finding all matches of a regular expression in a
multi-line text file:

<<parseReport>>=
report.alignments <- nc::capture_all_str(
  report.vec,
  "//",
  Alignment="[0-9]+",
  TSV="[^/]+")
class(report.alignments)
dim(report.alignments)
nchar(report.alignments$TSV)
substr(as.matrix(report.alignments[1:2]), 1, 50)

@

The code above computes a data table with ten rows, one for each match
in the text file. There are two columns: \code{Alignment} is a unique
identifier, and \code{TSV} contains the tab-separated values in each
block. Each \code{TSV} entry is a long text string (about 40,000
characters); we display the first 50 characters of the first two
blocks above. Because the result is a data table, we can parse the
\code{TSV} using a call to \code{fread} inside of a \code{by} block:

<<freadby>>=
(report.positions <- report.alignments[, data.table::fread(text=TSV), by=Alignment])
@

A bioinformatics analyst may desire to plot the \code{Likelihood} or
\code{Alpha} values in a genome browser, as a function of
\code{Position} along the chromosome. Each alignment has a chromosome
name which therefore needs to be added/joined to this table; the
second ``Info'' file has the chromosome name, as well as some other
summary statistics for each alignment:

<<info>>=
info.txt.gz <- system.file("extdata", "SweeD_Info.txt.gz", package="nc")
info.vec <- readLines(info.txt.gz)
info.vec[24:40]
@

Again, the data above can be parsed to a data table via a regular
expression:

<<infoparsed>>=
nc::capture_all_str(
  info.vec,
  " ",
  "Alignment", " ", Alignment="[0-9]+",
  "\n\n\t\t",
  "Chromosome", ":\t\t", Chromosome=".*")
@

The result above is a data table with ten rows, one for each
alignment. There are two columns: the first is the \code{Alignment}
identifier, and the second is the \code{Chromosome} name. Note the
repetition in the code above: \code{Alignment}/\code{Chromosome}
appear as capture group names as well as in the pattern
itself. Such repetition can be avoided by using the \code{nc::field} helper
function, which uses its first argument for the capture group name as
well as a pattern to match:

<<infoparsedfield>>=
(info.alignments <- nc::capture_all_str(
  info.vec,
  " ",
  nc::field("Alignment", " ", "[0-9]+"),
  "\n\n\t\t",
  nc::field("Chromosome", ":\t\t", ".*")))

@

Note that the repetition in the code has been removed, and the result
is the same (because the generated regex pattern is the same). Also
note that the order of the regex string literals is unchanged, so the
pattern is as easy to read/understand as the original repetitive
version. Internally, \code{nc::field} generates a pattern that
consists of pasting all of its arguments together; the first argument
is also used as the capture group name for the third (and subsequent)
arguments. In general, \code{nc::field} is useful whenever there are
several fields to parse, each of the form \code{variable=value}.

In order to create the desired output table with \code{Chromosome} and
\code{Position} columns, we join the two data tables:

<<join>>=
info.alignments[report.positions, on="Alignment"]
@

To conclude this section, we have shown that
\code{nc::capture\_all\_str} inputs a multi-line text file subject,
then outputs a data table with one row per match and one column per
capture group. We have also shown how these data tables may be further
processed (e.g. \code{fread}, \code{by=Alignment}) and joined using
the convenient data table syntax. Finally, we showed two examples of
using \code{nc::field} to define a pattern that matches/captures
fields of the form \code{variable=value}.

\subsection{Wide-to-tall data reshaping}

In this section we show how new \pkg{nc} functions can be used to
reshape wide data (with many columns) to tall data (with fewer
columns, and more rows). We begin by considering the two data
visualization problems mentioned in the introduction, involving the
familiar iris data set. First, suppose we would like to examine the
largest/smallest values or ranges. One way would be to use a histogram
of each numeric variable, with row facets for flower part and column
facets for measurement dimension. Our desired output therefore needs a
single column with all of the reshaped numeric data to plot
(Figure~\ref{fig:wide-to-tall}, W$\rightarrow$S). We can perform this
operation using \code{nc::capture\_melt\_single}, which inputs a data
frame and a pattern describing the columns to melt:

<<irisSingle>>=
(iris.tall.single <- nc::capture_melt_single(
  iris, part=".*", "[.]", dim=".*", value.name="cm"))
@

The output above consists of one copy column (\code{Species}), two
capture columns (\code{part}, \code{dim}), and a single reshape column
(\code{cm}). The \code{value.name} argument is not considered part of
the pattern, and instead specifies the name of the output reshape
column. These data can be used to create the desired histogram with
\pkg{ggplot2} via:

<<hist, fig=TRUE, height=2.2, width=5>>=
library(ggplot2)
ggplot(iris.tall.single)+facet_grid(part~dim)+
  theme_bw()+theme(panel.spacing=grid::unit(0, "lines"))+
  geom_histogram(aes(cm, fill=Species), color="black", bins=40)
@

For the second data reshaping task, suppose we want to determine
whether or not sepals are larger than petals, for each measurement
dimension and species. We could use a scatterplot of sepal versus
petal, with a facet for measurement dimension. We therefore need a
data table with two reshape output columns: a \code{Sepal} column to
plot against a \code{Petal} column (Figure~\ref{fig:wide-to-tall},
W$\rightarrow$M1). We can perform this operation using another
function, \code{nc::capture\_melt\_multiple}, which inputs a data
frame and a pattern which must contain the \code{column} group
and one other named group:

<<irisMultiple>>=
(iris.parts <- nc::capture_melt_multiple(
  iris, column=".*", "[.]", dim=".*"))
@

The output above consists of one copy column (\code{Species}), one
capture column (\code{dim}), and two reshape columns (\code{Petal},
\code{Sepal}). Note that each unique value captured in the special
\code{column} group becomes an output reshape column
name. These data can be used to create the scatterplot using
\pkg{ggplot2} via:

<<scatter, fig=TRUE, height=2, width=5>>=
ggplot(iris.parts)+facet_grid(.~dim)+
  theme_bw()+theme(panel.spacing=grid::unit(0, "lines"))+
  coord_equal()+geom_abline(slope=1, intercept=0, color="grey")+
  geom_point(aes(Petal, Sepal, color=Species), shape=1)
@

It is clear from the plot that sepals are larger than petals, for
both dimensions, and for every measured flower.

To conclude this section, \pkg{nc} provides two new functions for data
reshaping using regular expressions. Both functions input a data
frame to reshape, and a pattern to match to the column names. For
\code{nc::capture\_melt\_single}, all matching input columns are
reshaped in the output to a single column which is named using the
\code{value.name} argument. For \code{nc::capture\_melt\_multiple} the
output is multiple reshape columns with names defined by the values
captured in the special \code{column} group. Values from other
groups are stored in capture columns in the output. Both functions
support output of numeric capture columns via user-specified type
conversion functions.

\section{Comparison with other data reshaping packages}

In this section we compare the new data reshaping functions in the
\pkg{nc} package with similar functions in other packages. We aim to
demonstrate that the new \pkg{nc} syntax is often more powerful and
less repetitive.

\subsection{Advantages with respect to \pkg{tidyr}: pattern syntax and type conversion}

The \pkg{tidyr} package provides functions which can yield results
similar to \pkg{nc}. One advantage of \pkg{nc} is that complex
patterns may be defined in terms of simpler sub-patterns, which can
include group names and type conversion functions. Integrating these
three pieces results in a syntax that is easy to read as well; it is
more difficult to build and read complex patterns using \pkg{tidyr}
syntax, which requires specifying regex pattern strings, group names, and
types as separate arguments. Another advantage of \pkg{nc} is that the
range of possible type conversions for capture output columns is
essentially unlimited, since the user may provide an arbitrary type
conversion function for each group. In contrast the \pkg{tidyr} user
may specify a prototype for each group, which results in a limited
range of possible conversions. Of course, post-processing may be used,
but that introduces some repetition (of capture column names) in the
code. For example, consider a data set from the World Health
Organization:

<<who>>=
data(who, package="tidyr")
set.seed(1);sample(names(who), 10)

@

Each reshape column name starts with \code{new} and has three distinct
pieces of information: diagnosis type (e.g. \code{ep}, \code{rel}),
gender (\code{m} or \code{f}), and age range (e.g. \code{1524},
\code{4554}). We extract all three pieces of information below,
and include a function for converting gender to a factor with levels
in a specific (non-default) order:

<<whoreshape>>=
nc.who.sub.pattern <- list(
  "new_?", diagnosis=".*", "_",
  gender=".", function(mf)factor(mf, c("m", "f")))
nc.who.ages <- nc::capture_melt_single(who, nc.who.sub.pattern, ages=".*")
print(nc.who.ages[1:5], class=TRUE)
@

First note that \code{nc.who.sub.pattern} is a sub-pattern variable that we
have used as the first part of the pattern in the call to
\code{nc::capture\_melt\_single} above (and we will use that
sub-pattern again below). Sub-pattern lists may contain regex
character string literals (patterns to match), functions (for
converting the previous named group), or other sub-pattern lists. The
result is a data table with \code{gender} converted to a factor ---
this can also be accomplished in the \code{tidyr::pivot\_longer}
function:

<<pivot>>=
tidyr.who.sub.pattern <- "new_?(.*)_(.)"                   #L1
tidyr.who.pattern <- paste0(tidyr.who.sub.pattern, "(.*)") #L2
tidyr::pivot_longer(                                       #L3
  who, cols=grep(tidyr.who.pattern, names(who)),           #L4
  names_to=c("diagnosis", "gender", "ages"),               #L5
  names_ptypes=list(gender=factor(levels=c("m", "f"))),    #L6
  names_pattern=tidyr.who.pattern)[1:5,]                   #L7
@

In the code above we first define a sub-pattern variable for the
\code{diagnosis} and \code{gender} capture groups, as we did using
\pkg{nc}. One difference is that the \pkg{tidyr} pattern is a string
literal with un-named capture groups, whereas the \pkg{nc} pattern is
a list which includes capture group names as well as type conversion
functions. These three parameters are specified as three separate
arguments in \pkg{tidyr}, which results in some separation
(e.g. group names defined on L5 but sub-patterns are defined on L1)
and repetition (e.g. gender appears on L5 and L6) in the code. The
pattern also must be repeated: first in the \code{cols} argument (L4) to
specify the set of input columns to reshape, second in the
\code{names\_pattern} argument (L7) to specify how the output column values
should be captured from the input column names.

Now suppose we want to extract two numeric columns from \code{ages},
for example to use as interval-censored outputs in a survival
regression. Using \pkg{nc} we can use the previously defined
sub-pattern (including the previously defined group names and type
conversion function) as the first part of a larger pattern:

<<who2>>=
who.typed <- nc::capture_melt_single(
  who, nc.who.sub.pattern, ages=list(
    ymin="0|[0-9]{2}", as.numeric,
    ymax="[0-9]{0,2}", function(x)ifelse(x=="", Inf, as.numeric(x))))
who.typed[, .(rows=.N), by=.(ages, ymax, ymin)]

@

Note in the code above that each group name, regex pattern string, and
corresponding type conversion function appears on the same line ---
this syntax keeps these three related pieces of information close
together, which makes complex patterns easier to read and build from
smaller pieces. Also, note how an anonymous function is used to
convert the values captured in the \code{ymax} group to numeric (and
it maps the empty string to \code{Inf}). Such custom type conversion
functions are not supported by \pkg{tidyr}, so post-processing must be
used:


<<idyr2>>=
tidyr.who.pattern2 <- paste0(tidyr.who.sub.pattern, "((0|[0-9]{2})([0-9]{0,2}))")
transform(tidyr::pivot_longer(
  who, cols=grep(tidyr.who.pattern2, names(who)),
  names_to=c("diagnosis", "gender", "ages", "ymin", "ymax"),
  names_ptypes=list(gender=factor(levels=c("m", "f")), ymin=numeric()),
  names_pattern=tidyr.who.pattern2),
  ymax=ifelse(ymax=="", Inf, as.numeric(ymax)))[1:7,]
@

The code above uses \code{transform} as a post-processing step to
compute a numeric \code{ymax} column, which involves some repetition
(\code{ymax} appears four times). Note that numeric conversions which
do not require special logic (e.g. \code{ymin} above) can be specified
as prototypes using the \code{names\_ptypes} argument (but specifying
\code{ymax=numeric()} as a prototype results in a lossy cast error).

To conclude this comparison, we have seen that defining a complex
pattern is much more readable/understandable using \pkg{nc} syntax,
because it keeps group-specific names and type conversion functions
near the corresponding sub-patterns. We have also shown that
repetition is often necessary with \pkg{tidyr} (e.g. pattern, group
names), whereas repetition is avoided by using \pkg{nc}.

\subsection{Comparison with \pkg{data.table} and \code{stats::reshape}}

In this section we demonstrate the advantages of using \pkg{nc} over
\pkg{data.table} (which is used to implement \pkg{nc}
functionality). A major advantage is that \pkg{data.table} only
supports regular expressions for defining the set of input columns to
reshape; it does not support capture output columns. Another advantage
is that \pkg{nc} always returns a correct output data set with
multiple reshape columns, even when the input columns are not sorted
in a regular order. For example, consider the following simple data
set in which the columns are not in regular order:

<<iris2>>=
library(data.table)
(TC <- data.table(
  treatment.age=c(13, 15),
  control.gender=c("M", "M"),
  treatment.gender=c("F", "F"),
  control.age=c(25, 22)))
@

It is clear from the table above that the treatment group consists of
teenage females, whereas the control group consists of males in their
twenties (not the best experimental design, but easy to remember for
the demonstration in this section). Assume we need an output data
table with two reshape columns (\code{age} and \code{gender}) as well
as a capture column (\code{group}). The \pkg{nc} syntax we would use
is:

<<ncgroups>>=
nc::capture_melt_multiple(TC, group=".*", "[.]", column=".*")
@

The correct result is computed above because \pkg{nc} reshapes based
on the input column names (column order is not relevant). A na\"ive
user may attempt to perform this reshape using
\code{data.table::patterns}:

<<patterns>>=
melt(TC, measure.vars=patterns(age="age", gender="gender"))
@

First, note that the syntax above requires repetition of \code{age}
and \code{gender} (in names and in pattern strings). Also it is clear
that the result is incorrect!  Actually, \code{patterns} is working as
documented; it ``returns the matching indices'' of the provided
regex. However, since the input columns are not sorted in regular
order, \code{melt} returns an incorrect result. To get a correct
result, we can provide a list of index vectors:

<<oo>>=
melt(TC, measure.vars=list(age=c(1,4), gender=c(3,2)))
@

This is what \pkg{nc} does internally; it also converts the
\code{variable} output column to a more interpretable/useful capture
column (e.g. \code{group} above).

The \code{stats::reshape} function suffers from the same
issue. Another issue with this function is that it assumes the output
reshape column names are the first part of the input column names
(e.g. Figure~\ref{fig:wide-to-tall}, W$\rightarrow$M1). When input
column names have a different structure
(e.g. Figure~\ref{fig:wide-to-tall}, W$\rightarrow$M2), they must be
renamed, putting the desired output reshape
column names first:

<<reshapenames>>=
TC.renamed <- structure(TC, names=sub("(.*)[.](.*)", "\\2.\\1", names(TC)))
stats::reshape(TC.renamed, 1:4, direction="long", timevar="group")
@

However the result above still contains incorrect results in the
\code{gender} column. The correct result can be obtained by sorting
the input column names:

<<reshapesort>>=
TC.sorted <- data.frame(TC.renamed)[, sort(names(TC.renamed))]
stats::reshape(TC.sorted, 1:4, direction="long", timevar="group")
@

After renaming and sorting the input columns, the correct result is
obtained using \code{stats::reshape}. Another way to obtain a
correct result is with the \pkg{cdata} package:

<<cdata>>=
cdata::rowrecs_to_blocks(TC, controlTable=data.frame(
  group=c("treatment", "control"),
  age=c("treatment.age", "control.age"),
  sex=c("treatment.gender", "control.gender"),
  stringsAsFactors=FALSE))
@

The \pkg{cdata} package is very powerful, and can handle many
more types of data reshaping operations than \pkg{nc}. However, it
requires a very explicit definition of the desired conversion in terms
of a control table, which results in rather verbose code. In contrast,
the terse regular expression syntax of \pkg{nc} is a more implicit
approach, which assumes the input columns to reshape have regular
names.

To conclude this section, we have discussed some advantages of
\pkg{nc} relative to other R packages. Regular expressions can be used
in \pkg{nc} to specify capture output columns, which are not provided
in the results from other functions such as
\code{data.table::melt}. Input columns with regular names do not need
to be renamed/sorted for \pkg{nc} functions, whereas renaming/sorting
may be necessary using other functions (\code{data.table::melt},
\code{stats::reshape}). Verbose/explicit control table code is always necessary
with \pkg{cdata}, whereas a terse/implicit regular expression syntax is
used with \pkg{nc} to simplify the definition of reshape operations.

\subsection{Timings}

\begin{figure}
  \centering
  \includegraphics[width=0.49\textwidth]{figure-who-rows}
  \includegraphics[width=0.49\textwidth]{figure-who-cols}
  \caption{Timings}
  \label{fig:timings}
\end{figure}

\section{Discussion and conclusions}

TODO One versus two functions.

TODO arbitrary type conversion.

\paragraph{Reproducible research statement.} The source code for this
article can be freely downloaded from
\url{https://github.com/tdhock/nc-article}

\bibliography{hocking}

\address{Toby Dylan Hocking\\
  School of Informatics, Computing, and Cyber Systems\\
  Northern Arizona University\\
  Flagstaff, Arizona\\
  USA\\
  \email{toby.hocking@nau.edu}}

